{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmiXZzivobao"
      },
      "source": [
        "#Продвинутый Python, ДЗ-7\n",
        "\n",
        "Правила игры:\n",
        "\n",
        "В домашке 1 задача. Суммарно за дз можно получить 100 баллов, что равняется 10 баллам\n",
        "\n",
        "Дедлайн - 7 дней после выдачи дз. Необходимо залить решеннный ноутбук в github и прислать ссылку в Anytask (без выполнения любого из пунктов работа проверяться не будет)\n",
        "\n",
        "Оценка за ДЗ складывается следующим образом:\n",
        "\n",
        "* наличие полного результата - 10 баллов\n",
        "\n",
        "* наличие только 1 запроса - -4 балла\n",
        "\n",
        "* отсутствие фильтрации - -2 балла\n",
        "\n",
        "* запросы только с первой страницы - -2 балла\n",
        "\n",
        "* не все поля вытащены - -2 балла\n",
        "\n",
        "В качестве результата добавляйте в репозиторий полностью папку с пауком и всеми необходимыми файлами\n",
        "\n",
        "**N.B.** Steam спокойно парсится без посторонних инструментов обхода блокировок"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kAwYv9PrIJz"
      },
      "source": [
        "## Задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yowliMx-rtYN"
      },
      "source": [
        "Парсим [Steam](https://store.steampowered.com/) с помощью Scrapy!\n",
        "\n",
        "Необходимо выбрать 3 запроса (например, \"инди\", \"стратегии\", \"minecraft\", на каждый запрос должно быть минимум 90 результатов), и вытащить по ним:\n",
        "\n",
        "* Все игры на первых 2 страницах (найдите, каким образом отображаются страницы)\n",
        "\n",
        "И для каждой игры вытащить:\n",
        "\n",
        "* Название\n",
        "\n",
        "* Категорию (весь путь, за исключением Все игры и самого названия игры)\n",
        "\n",
        "* Число всех обзоров и общая оценка\n",
        "\n",
        "* Дата выхода\n",
        "\n",
        "* Разработчик\n",
        "\n",
        "* Метки (тэги) игры\n",
        "\n",
        "* Цена\n",
        "\n",
        "* Доступные платформы\n",
        "\n",
        "Сохраните игры в формате json, оставив только игры, выпущенные после 2000 года\n",
        "\n",
        "Чтобы было проще, в репозитории с ДЗ выложены скрины, где можно посмотреть, где и какие поля находятся\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMZ7VvoxX1eF"
      },
      "outputs": [],
      "source": [
        "!pip install scrapy\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "import json\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RNKyD_PVaruv"
      },
      "outputs": [],
      "source": [
        "PAGES_NUM = 2\n",
        "# 19 = action\n",
        "# 492 = indie\n",
        "# 599 = simulators\n",
        "STEAM_CATEGORIES = [492,19,599]\n",
        "\n",
        "YEAR_NEWER_THAN = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z8hDvHwvyIRa"
      },
      "outputs": [],
      "source": [
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('result.json', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        if (int(item['published'].split()[-1]) >= YEAR_NEWER_THAN):\n",
        "            line = json.dumps(dict(item)) + \"\\n\"\n",
        "            self.file.write(line)\n",
        "        return item\n",
        "\n",
        "class SteamGameItem(scrapy.Item):\n",
        "    title = scrapy.Field()\n",
        "    price = scrapy.Field()\n",
        "    published = scrapy.Field()\n",
        "    platforms = scrapy.Field()\n",
        "    breadcrumb = scrapy.Field()\n",
        "    developer = scrapy.Field()\n",
        "    reviews = scrapy.Field()\n",
        "    tags = scrapy.Field()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3OjpEHvmrd_W"
      },
      "outputs": [],
      "source": [
        "class SteamSpider(scrapy.Spider):\n",
        "    name = \"steam\"\n",
        "    start_urls = [\n",
        "        'https://store.steampowered.com/search?tags=%d&page=%d' % (c,n) for c in STEAM_CATEGORIES for n in range(1,PAGES_NUM+1)\n",
        "    ]\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1},\n",
        "        'FEED_FORMAT':'json',\n",
        "        'FEED_URI': 'quoteresult.json',\n",
        "        'AUTOTHROTTLE_ENABLED': True,\n",
        "        'AUTOTHROTTLE_TARGET_CONCURRENCY': 4.0,\n",
        "        'HTTPCACHE_ENABLED': True\n",
        "    }\n",
        "\n",
        "    def parse(self, response):\n",
        "        divs = response.css(\".responsive_search_name_combined\")\n",
        "        for div in divs:\n",
        "            item = SteamGameItem()\n",
        "            link_to_game = div.xpath('../@href').get()\n",
        "            item['title'] = div.css('span.title ::text').get()\n",
        "            item['published'] = div.css('div.search_released ::text').get().strip()\n",
        "            item['price'] = div.css('div.discount_final_price ::text').get()\n",
        "            item['platforms'] = ['%s' % pl.join(pl.split(' ')[1:]) for pl in div.css('span.platform_img ::attr(\"class\")').getall()]\n",
        "            yield response.follow(url=link_to_game,callback=self.parse_game,meta = {'item':item})\n",
        "\n",
        "    def parse_game(self, response):\n",
        "        item = response.meta['item']\n",
        "        item['breadcrumb'] = [_ for _ in [_.replace('>','').strip() for _ in response.css('div.breadcrumbs > div.blockbg ::text').getall()] if _ != ''][1:-1]\n",
        "        item['developer'] = [_ for _ in [_.replace(',','').strip() for _ in response.css('div.summary#developers_list ::text').getall()] if _ != '']\n",
        "        item['tags'] = [_ for _ in [_.replace('+','').strip() for _ in response.css('div.popular_tags ::text').getall()] if _ != '']\n",
        "        item['reviews'] = response.css('meta[itemprop=\"reviewCount\"]::attr(content)').get()\n",
        "        yield item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7AIWCepmERg"
      },
      "outputs": [],
      "source": [
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "process.crawl(SteamSpider)\n",
        "process.start()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
